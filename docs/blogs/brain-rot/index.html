<!DOCTYPE html>
<html
  lang="en"
  dir="ltr"
  
><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>The Collapsing Mind: How Language Topology Shapes Our Imagination | Yifan Wang</title>

<meta name="generator" content="Hugo Eureka 0.9.3" />
<link rel="stylesheet" href="https://cacayaya.github.io/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css">
<script defer src="https://cacayaya.github.io/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js"></script>













<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&amp;family=Noto&#43;Serif&#43;SC:wght@400;600;700&amp;display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js"
   crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js"
     crossorigin></script>
<link rel="stylesheet" href="https://cacayaya.github.io/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css" media="print" onload="this.media='all';this.onload=null">


<script defer type="text/javascript" src="https://cacayaya.github.io/js/fontawesome.min.1853a524ca39729ab933ec9ed36c061ff97ab42d22bfb99eebbff3b8df0476a351eea2822c6447a461ca0fbc56183b11.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
   integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" 
  integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
   integrity="sha384-&#43;XBljXPPiv&#43;OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js" 
  integrity="sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0"  crossorigin></script>


<link rel="icon" type="image/png" sizes="32x32" href="https://cacayaya.github.io/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://cacayaya.github.io/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_3.png">

<meta name="description"
  content="LLMs Can Get “Brain Rot” Recently, we published a paper examining what happens when large language models consume too much highly popular but low quality content such as viral tweets and short social posts. The results were striking: reasoning ability fell by 23 percent, long-context memory decreased by 30 percent, and simulated personality tests showed spikes in narcissism and psychopathy. Even after retraining on clean, high-quality data, the model struggled to fully recover.
We call this phenomenon “LLM Brain Rot”, as it closely resembles what can happen to the human mind when it is continuously exposed to shallow, repetitive, and emotionally charged information.">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Blogs",
      "item":"https://cacayaya.github.io/blogs/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"The Collapsing Mind: How Language Topology Shapes Our Imagination",
      "item":"https://cacayaya.github.io/blogs/brain-rot/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://cacayaya.github.io/blogs/brain-rot/"
    },
    "headline": "The Collapsing Mind: How Language Topology Shapes Our Imagination | Yifan Wang","datePublished": "2025-10-28T21:05:36-04:00",
    "dateModified": "2025-10-28T21:05:36-04:00",
    "wordCount":  485 ,
    "publisher": {
        "@type": "Person",
        "name": "Yifan Wang",
        "logo": {
            "@type": "ImageObject",
            "url": "https://cacayaya.github.io/images/icon.png"
        }
        },
    "description": "LLMs Can Get “Brain Rot” Recently, we published a paper examining what happens when large language models consume too much highly popular but low quality content such as viral tweets and short social posts. The results were striking: reasoning ability fell by 23 percent, long-context memory decreased by 30 percent, and simulated personality tests showed spikes in narcissism and psychopathy. Even after retraining on clean, high-quality data, the model struggled to fully recover.\nWe call this phenomenon “LLM Brain Rot”, as it closely resembles what can happen to the human mind when it is continuously exposed to shallow, repetitive, and emotionally charged information."
}
</script><meta property="og:title" content="The Collapsing Mind: How Language Topology Shapes Our Imagination | Yifan Wang" />
<meta property="og:type" content="article" />


<meta property="og:image" content="https://cacayaya.github.io/images/icon.png">


<meta property="og:url" content="https://cacayaya.github.io/blogs/brain-rot/" />




<meta property="og:description" content="LLMs Can Get “Brain Rot” Recently, we published a paper examining what happens when large language models consume too much highly popular but low quality content such as viral tweets and short social posts. The results were striking: reasoning ability fell by 23 percent, long-context memory decreased by 30 percent, and simulated personality tests showed spikes in narcissism and psychopathy. Even after retraining on clean, high-quality data, the model struggled to fully recover.
We call this phenomenon “LLM Brain Rot”, as it closely resembles what can happen to the human mind when it is continuously exposed to shallow, repetitive, and emotionally charged information." />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="Yifan Wang" />






<meta property="article:published_time" content="2025-10-28T21:05:36-04:00" />


<meta property="article:modified_time" content="2025-10-28T21:05:36-04:00" />



<meta property="article:section" content="blogs" />





  <body class="flex min-h-screen flex-col">
    <header
      class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"
    >
      <div class="mx-auto w-full max-w-screen-xl"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="me-6 text-primary-text text-xl font-bold">Yifan Wang</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">ABOUT</a>
            <a href="/projects/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">PROJECTS</a>
            <a href="/blogs/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  me-4">BLOGS</a>
            <a href="/cv/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">CV</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">Light</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
    </header>
    <main class="grow pt-16">
        <div class="pl-scrollbar">
          <div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8">
  
  
  <div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12">
    <div
      class="lg:col-start-2 bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"
    >
      <article class="prose">
  <h1 class="mb-4">The Collapsing Mind: How Language Topology Shapes Our Imagination</h1>

  <div
  class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"
>
  <div class="me-6 my-2">
    <i class="fas fa-calendar me-1"></i>
    <span
      >2025-10-28</span
    >
  </div>
  <div class="me-6 my-2">
    <i class="fas fa-clock me-1"></i>
    <span>3 min read</span>
  </div>

  

  
</div>


  
  

  <!-- 
### human / LLM brain rot
Recently we published a paper talking about when LLM consume too much tweet junk data (short but
highly popular posts), it will get brain-rot:
- Reasoning fell by 23%
- Long-context memory dropped 30%
- Personality tests showed spikes in narcissism & psychopathy
And get this even after retraining on clean, high-quality data, the damage didn’t fully heal.

Another or more interesting thing I am thinking is about human model collapse, since so much of our human training data is now derivative/recursively generated (aggregated news, various retellings of stories across social media, etc) and seems to be having a similar effect on us as low-quality synthetic data has on LLMs. 
And another interesting paper(https://arxiv.org/pdf/2007.09560) mentions that dreaming can assit the overfitted brain. We often superised by strange talk by childrens(underfitted brain) as well as wild imagination of our dream. So yes, perhaps normal day life of an adult is a collapsed model doing things mechanically. We saw after a stage people will tend to be more stubborn and no longer want to absorb different aspects regarding the world, maybe is a sign of gradually overfitted a small set of data(your experience) and find your local minima without tempting to climb out of it.

### reason / Topology of language
语言表征的过程既有不变性,也有可变性,这种变化中的不变性质,就是语言表征的拓扑性质。
语言和物理世界处于同胚( homeomohism) 共相的关系;
语言的种种表征是基于共相的物理世界的同胚变形，即便是语言表达经过改动、变通、删减等一系列转化和修正后，语言主体仍能理解所刻画的物理世界这一恒定不变的常量。

这是一个很有趣的概念，这可能意味着人类进行语言操作的时候是在锻炼脑内进行拓扑运算，另外想到看视频和看书相比，在脑子里面进行拓扑运算的机会少很多，因为在视频里面的拓扑形式是由其几何直接显现，然而这就使人丧失了想象力，因为想象力显然也是拓扑的：想象力即是从语言的拓扑结构重建几何图像的能力。

And this will related to why short videos or various retellings of stories across social media will leads to human brain-rot, that's because those material cannot 锻炼我们进行拓扑计算的能力，同时limit我们的想象力，理解力。


而且这种情况可能正在被AI加速，在之前只是Human produce almost same information，但是AI continue produce much more 同质化内容并且充斥互联网，这可能使得普通人获得的information和锻炼更加垃圾

https://cstj.cqvip.com/Qikan/Article/Detail?id=668285357&from=Qikan_Search_Index
https://terpconnect.umd.edu/~israel/lakoff-ConTheorMetaphor.pdf
 -->
<h3 id="llms-can-get-brain-rot">LLMs Can Get “Brain Rot”</h3>
<p>Recently, we published a <a href="https://arxiv.org/abs/2510.13928">paper</a> examining what happens when large language models consume too much highly popular but low quality content such as viral tweets and short social posts. The results were striking: reasoning ability fell by 23 percent, long-context memory decreased by 30 percent, and simulated personality tests showed spikes in narcissism and psychopathy. Even after retraining on clean, high-quality data, the model struggled to fully recover.</p>
<p>We call this phenomenon “LLM Brain Rot”, as it closely resembles what can happen to the human mind when it is continuously exposed to shallow, repetitive, and emotionally charged information.</p>
<h3 id="humans-can-experience-model-collapse">Humans Can Experience “Model Collapse”</h3>
<p>The analogy also works in the opposite direction. Humans can also experience “Model Collapse”.</p>
<p>Children, whose brains are still underfitted, surprise us with wild imagination and unpredictable speech. Adults, by contrast, optimize for efficiency and stability. We become repetitive learners, confident in limited experience, reluctant to explore new dimensions of thought. Perhaps the everyday life of an adult mirrors a collapsed model, converged too early on a narrow set of experiences and repeating the same outputs without further learning. Over time, many people grow more stubborn and less receptive to new perspectives, a sign of slow cognitive overfitting: excessive dependence on a small personal dataset and a preference for the comfort of a local minimum rather than the effort of climbing out of it.</p>
<p>An interesting paper <a href="(https://arxiv.org/pdf/2007.09560)">The Overfitted Brain (2020)</a> provides a compelling clue. He suggests that dreaming acts as a regularization mechanism for the overfitted mind, injecting randomness and abstraction to restore flexibility.</p>
<h3 id="the-topology-of-language-and-the-erosion-of-imagination">The Topology of Language and the Erosion of Imagination</h3>
<p>When we dig why human more easily got brain rot today, one interesting perspective is to study the nature of human language.</p>
<p>Language representation contains both variability and invariance. The stability that persists within change reflects a topological property. Language and the physical world can be viewed as homeomorphic, meaning they share a structural correspondence. Even when linguistic forms are modified, paraphrased, or reduced, they still refer to the same underlying reality.</p>
<p>This idea implies that understanding language requires topological computation inside the brain. When we read, we are actively performing these transformations, reshaping form while preserving meaning. Reading provides the topology, and our mind reconstructs the geometry. Watching videos, in contrast, bypasses this process. The geometric form is presented directly, leaving little room for internal reconstruction. Over time, the brain loses opportunities to practice its own topological operations. This may weaken imagination and comprehension, both of which depend on the ability to rebuild geometry from abstract representation.</p>
<p>Short videos and repetitive social content therefore contribute to what might be called human brain rot. They fail to exercise our topological reasoning and gradually compress our imaginative range. AI now amplifies this tendency toward linguistic centralization. By favoring the most probable words and phrases, it compresses our expressive space and makes language increasingly uniform.</p>

</article>


      

      



      

      
  <div
    class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"
  >
    <div>
      
    </div>
    <div class="mt-4 md:mt-0 md:text-right">
      
        <span class="text-primary-text block font-bold">Next</span>
        <a href="https://cacayaya.github.io/blogs/freedom/" class="block">Freedom, Intelligence and Agent</a>
      
    </div>
  </div>


      



    </div>
    

    
    
  </div>

  
    <script>
      document.addEventListener("DOMContentLoaded", () => {
        hljs.highlightAll();
      });
    </script>

          </div>
        </div>
      
    </main>
    <footer class="pl-scrollbar">
      <div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2023 Yifan Wang 
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
    </footer>
  </body>
</html>
