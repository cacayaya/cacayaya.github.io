<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yifan Wang</title>
    <link>https://cacayaya.github.io/authors/yifan-wang/</link>
    <description>Recent content on Yifan Wang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; 2023 Yifan Wang 
</copyright>
    <lastBuildDate>Fri, 02 Aug 2024 11:16:51 -0500</lastBuildDate>
    <atom:link href="https://cacayaya.github.io/authors/yifan-wang/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cascade reward sampling for efficient decoding-time alignment</title>
      <link>https://cacayaya.github.io/projects/cards/</link>
      <pubDate>Fri, 02 Aug 2024 11:16:51 -0500</pubDate>
      <guid>https://cacayaya.github.io/projects/cards/</guid>
      <description>Aligning large language models (LLMs) with human preferences is critical for their deployment. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that requires no fine-tuning of model parameters. However, generating text that achieves both high reward and high likelihood remains a significant challenge. Existing methods often fail to generate high-reward text or incur substantial computational costs. In this paper, we propose Cascade Reward Sampling (CARDS) to address both issues, guaranteeing the generation of high-reward and high-likelihood text with significantly low costs.</description>
    </item>
    <item>
      <title>A Theory of Fault-Tolerant Learning</title>
      <link>https://cacayaya.github.io/projects/a-theory-of-fault-tolerant-learning/</link>
      <pubDate>Sat, 25 May 2024 16:31:26 -0400</pubDate>
      <guid>https://cacayaya.github.io/projects/a-theory-of-fault-tolerant-learning/</guid>
      <description>Developing machine learning models that account for potential faults encountered in real-world environments presents a fundamental challenge for mission-critical applications. In this paper, we introduce a novel theoretical framework grounded in learning theory for dealing with faults. In particular, we propose a framework called fault-tolerant PAC learning, aimed at identifying the most fault-tolerant models from a given hypothesis class (such as neural networks). We show that if faults occur randomly, fault-tolerant learning is equivalent to regular PAC learning.</description>
    </item>
    <item>
      <title>Learning Functional Distributions with Private Labels</title>
      <link>https://cacayaya.github.io/projects/learning_functional_distributions/</link>
      <pubDate>Tue, 23 May 2023 16:31:26 -0400</pubDate>
      <guid>https://cacayaya.github.io/projects/learning_functional_distributions/</guid>
      <description>We study the problem of learning functional distributions in the presence of noise. The functional is a map from features to distributions over a set of labels and is assumed to belong to a known class of hypotheses. Features are generated by a general random process and labels are sampled independently from the feature-dependent distributions and then passed through a noisy kernel. We consider online learning where at each time step a predictor attempts to predict the actual (label) distribution given only the features revealed so far and noisy labels in prior steps.</description>
    </item>
  </channel>
</rss>
